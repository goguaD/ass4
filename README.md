მონაცემთა პრეპროცესინგი
მონაცემთა განაწილება: კოდი იწყება მონაცემთა ნაკრების განაწილების ანალიზით (ტრენინგი, ვალიდაცია, ტესტი).

ემოციების ლეიბლები: განსაზღვრულია 7 კლასი შესაბამისი ემოციებით.

ვიზუალიზაცია: ფუნქცია plot_sample_images აჩვენებს მონაცემთა ნაკრების შემთხვევით ნიმუშებს.

FERDataset კლასი: ადგენს მონაცემთა ჩატვირთვის ლოგიკას PyTorch-ისთვის.

მოდელის არქიტექტურა
CNN მოდელი შედგება:

4 კონვოლუციური ფენისგან (64, 128, 256, 512 ფილტრით)

Batch normalization და ReLU აქტივაციით

Max pooling ფენებით

2 სრულად უკავშირებული ფენისგან კლასიფიკაციისთვის

ტრენინგის პროცესი
პარამეტრები: გამოყენებულია Adam ოპტიმიზატორი 0.001 სწავლის სიჩქარით.

რეგულარიზაცია: Dropout (0.5) და weight decay (1e-5).

სასწავლო სიჩქარის ადაპტაცია: ReduceLROnPlateau სკედიულერი.

მონიტორინგი: Weights & Biases (W&B) ინსტრუმენტის გამოყენება.

ევალუაცია
ტესტირება: საუკეთესო მოდელის შერჩევა validation accuracy-ის მიხედვით.

კონფუზიის მატრიცა: ვიზუალიზაცია თითოეული კლასისთვის.

მეტრიკები: precision, recall, F1-score თითოეული ემოციისთვის.

ოპტიმიზაციები და შედარება
ბაზური მოდელი:

მარტივი CNN არქიტექტურა

ძირითადი ტრანსფორმაციები (flip, rotation)

ძირითადი რეგულარიზაცია (dropout)

ოპტიმიზაცია #1 - არქიტექტურა:

უფრო ღრმა ქსელი დამატებითი კონვოლუციური ფენებით

Batch normalization ყოველი კონვოლუციური ფენის შემდეგ

შედეგი: ~3-5% accuracy-ის გაზრდა

ოპტიმიზაცია #2 - რეგულარიზაცია:

მეტი dropout ფენები

Weight decay პარამეტრის დამატება

შედეგი: ნაკლები overfitting, უფრო სტაბილური ტრენინგი

ოპტიმიზაცია #3 - სასწავლო პროცესი:

სასწავლო სიჩქარის ადაპტაცია

Early stopping პრინციპის გამოყენება

შედეგი: უკეთესი კონვერგენცია, ~2% accuracy-ის გაზრდა

ოპტიმიზაცია #4 - მონაცემთა აგმენტაცია:

მეტი შემთხვევითი ტრანსფორმაციები

შედეგი: უკეთესი გენერალიზაცია, ~4% accuracy-ის გაზრდა validation set-ზე

საბოლოო მოდელი აღწევს ~65-70% accuracy-ს ტესტურ მონაცემებზე, რაც მნიშვნელოვანი გაუმჯობესებაა ბაზური ვერსიის (~55-60%) მიმართ.

მეორე მოდელის აღწერა (SGD ოპტიმიზატორით)
რა შეცვალე?
პირველი მოდელიდან განსხვავებით, ამ ვერსიაში:

Adam ოპტიმიზატორის ნაცვლად გამოვიყენე SGD (Stochastic Gradient Descent)

დავტოვე იგივე CNN არქიტექტურა და ყველა სხვა პარამეტრი

რა მოხდა?
Accuracy დაეცა 49%-მდე (პირველ მოდელთან შედარებით ~15-20%-ით ქვევით)

ტრენინგის პროცესი გახდა უფრო ნელი და არასტაბილური

მოდელმა ვერ ისწავლა კარგი features-ის ამოღება

რატომ მოხდა ეს?
SGD-ის ნაკლოვანებები:

არ აქვს ადაპტირებადი სწავლის სიჩქარე (როგორც Adam-ს)

უფრო მგრძნობიარეა ცუდი პარამეტრების არჩევის მიმართ

პრაქტიკული გაკვეთილი:

Adam ოპტიმიზატორი უკეთესად მუშაობს ამ ტიპის CNN ამოცანებში

SGD-ს სჭირდება ძალიან ფრთხილად შერჩეული სწავლის სიჩქარე და მომენტუმის პარამეტრები

ეს ექსპერიმენტი აჩვენებს, რომ ოპტიმიზატორის არჩევანი შეიძლება მნიშვნელოვნად გავლენა იქონიოს მოდელის პროდუქტიულობაზე.

მესამე მოდელის აღწერა
თქვენს მიერ მოწოდებული მესამე მოდელი წარმოადგენს მნიშვნელოვნად გაუმჯობესებულ Convolutional Neural Network (CNN) არქიტექტურას სახის ემოციების ამოცნობისთვის. მან მიაღწია დაახლოებით 66% სიზუსტეს (accuracy), რაც უკეთესია თქვენს წინა მოდელებთან შედარებით.

რატომ არის ეს მოდელი განსხვავებული და რატომ მიაღწია 66%-ს?
ეს მოდელი წინა ვერსიებისგან განსხვავდება რამდენიმე კრიტიკული ოპტიმიზაციით, რომლებიც მიზნად ისახავდა მოდელის სწავლის უნარის გაუმჯობესებას, რთული მახასიათებლების ამოღებას და განზოგადების გაძლიერებას.

ძირითადი ცვლილებები და გაუმჯობესებები:

ღრმა არქიტექტურა ImprovedFaceCNN: მოდელი გახდა უფრო ღრმა, გაიზარდა კონვოლუციური ფენების რაოდენობა (1-დან 4-მდე: 64, 128, 256, 512 ფილტრი). ეს საშუალებას აძლევს მოდელს ისწავლოს უფრო რთული და იერარქიული მახასიათებლები გამოსახულებებიდან, რაც გადამწყვეტია სახის ემოციების ნიუანსების დასაფიქსირებლად.
Batch Normalization (BN): Batch Normalization ფენების დამატება ყოველი კონვოლუციური ფენის შემდეგ სტაბილიზაციას უკეთებს ტრენინგის პროცესს. ის ამცირებს შიდა კოვარიანტულ ცვლას (internal covariate shift) და საშუალებას აძლევს მოდელს ისწავლოს უფრო სწრაფად და ეფექტურად.
Leaky ReLU აქტივაციის ფუნქცია: Leaky ReLU-ის გამოყენება სტანდარტული ReLU-ს ნაცვლად ეხმარება "მკვდარი ReLU" პრობლემის თავიდან აცილებაში, რაც ხელს უწყობს გრადიენტების უკეთ გავრცელებას და ქსელის უფრო ეფექტურ სწავლას.
Skip Connection (ნარჩენი კავშირი): skip ბლოკის დამატება საშუალებას აძლევს ინფორმაციას, გვერდი აუაროს კონკრეტულ ფენებს და უშუალოდ გადავიდეს უფრო ღრმა ფენებში. ეს ხელს უწყობს გრადიენტების გადინებას, ხელს უშლის ინფორმაციის დაკარგვას და ეხმარება მოდელს უფრო ღრმად ისწავლოს. ეს არის ResNet-ის მსგავსი არქიტექტურული პრინციპი.
Global Average Pooling (GAP): სრულად დაკავშირებული ფენების ნაცვლად GAP-ის გამოყენება კონვოლუციური ბლოკების შემდეგ ამცირებს პარამეტრების რაოდენობას, რაც ამცირებს overfitting-ის რისკს და მოდელს უფრო მტკიცეს ხდის.
გაუმჯობესებული სრულად დაკავშირებული ფენები (FC): საბოლოო კლასიფიკაციისთვის გამოყენებულია ორ ფენიანი სრულად დაკავშირებული ბლოკი (512 -> 256 -> 7), რომელშიც ჩართულია Batch Normalization და Dropout ფენები. ეს უზრუნველყოფს უკეთეს რეგულარიზაციას და აუმჯობესებს მოდელის განზოგადების უნარს.
Adam ოპტიმიზატორი Weight Decay-ით: ისევ დაუბრუნდით Adam ოპტიმიზატორს, მაგრამ ამჯერად დამატებით weight_decay=1e-5 რეგულარიზაციით. Weight decay ხელს უწყობს მოდელის კომპლექსურობის შემცირებას და overfitting-ის თავიდან აცილებას.
ReduceLROnPlateau Scheduler: სწავლის სიჩქარის დინამიური ადაპტაცია ReduceLROnPlateau სკედიულერის საშუალებით, რომელიც ამცირებს სწავლის სიჩქარეს, როდესაც ვალიდაციის სიზუსტე აღარ უმჯობესდება. ეს ხელს უწყობს მოდელის უკეთეს კონვერგენციას და ხელს უშლის სწავლის სიჩქარის ნაადრევად დაფიქსირებას.
W&B ინტეგრაცია: Weights & Biases-ის აქტიური გამოყენება ტრენინგის პროცესის დეტალური მონიტორინგისთვის, რაც საშუალებას იძლევა ადვილად თვალყური ადევნოთ და გააანალიზოთ ისეთი მეტრიკები, როგორიცაა loss, accuracy და learning rate.
ამ ოპტიმიზაციების კომბინაციამ, განსაკუთრებით გაუმჯობესებულმა არქიტექტურამ (უფრო ღრმა ფენები, Batch Normalization, Skip Connection) და ტრენინგის უკეთესმა სტრატეგიამ (Adam with weight decay, LR scheduler), მოდელს საშუალება მისცა უკეთესად ისწავლოს მონაცემებიდან და მიაღწიოს 66% სიზუსტეს.

მეოთხე მოდელის აღწერა
(GPU აღარ მქონდა მაგრამ 70% ამდე ავიდა).
თქვენს მიერ მოწოდებული ეს მოდელი, AdvancedFaceCNN, წარმოადგენს შემდგომ ოპტიმიზაციას სახის ემოციების ამოცნობის პროექტში. მან მიაღწია 65% სიზუსტეს (accuracy) ტესტურ მონაცემებზე.

რა შეცვალე წინა მოდელთან შედარებით და რატომ მიიღო 65%-იანი შედეგი?
ეს ვერსია შეიცავს მნიშვნელოვან ცვლილებებს მოდელის არქიტექტურაში, მონაცემთა აგმენტაციასა და ტრენინგის სტრატეგიაში.

ძირითადი განსხვავებები და ცვლილებები:

მოდელის არქიტექტურა (AdvancedFaceCNN):
უფრო სტრუქტურირებული ბლოკები: კონვოლუციური ფენები (64, 128, 256, 512 ფილტრებით) დაჯგუფებულია features ბლოკებად, თითოეული შეიცავს ორ კონვოლუციურ ფენას Batch Normalization-ით, LeakyReLU აქტივაციით, Max Pooling-ით და Dropout-ით.
Skip Connection-ის მოხსნა: წინა ImprovedFaceCNN მოდელისგან განსხვავებით, ამ ვერსიაში აღარ გამოიყენება "Skip Connection" (ნარჩენი კავშირი). ეს შეიძლება იყოს ერთ-ერთი მიზეზი, რის გამოც სიზუსტე ოდნავ შემცირდა, რადგან ღრმა ქსელებში Skip Connection-ები გრადიენტების უკეთ გავრცელებას უწყობს ხელს.
გაუმჯობესებული Dropout: Dropout ფენები (0.25) ინტეგრირებულია features ბლოკებშიც, რაც აძლიერებს რეგულარიზაციას.
მონაცემთა აგმენტაცია:
გაფართოებული ტრანსფორმაციები: დამატებულია ახალი, უფრო აგრესიული აგმენტაციები, როგორიცაა transforms.RandomAffine (შემთხვევითი წანაცვლება), transforms.GaussianBlur (გაუსისებრი დაბინდვა) და transforms.RandomErasing (შემთხვევითი წაშლა). ეს გაუმჯობესებები მოდელს ეხმარება უკეთ განაზოგადოს მონაცემები სხვადასხვა პირობებში, თუმცა ზოგჯერ ზედმეტმა აგრესიულობამ შეიძლება ოდნავ გაართულოს სწავლის პროცესი.
ნორმალიზაცია: mean და std მნიშვნელობები შეიცვალა [0.485] და [0.229]-ით.
სწავლის პროცესის ოპტიმიზაცია:
Loss Function (label_smoothing): გამოყენებულია nn.CrossEntropyLoss label_smoothing=0.1 პარამეტრით. ეს ტექნიკა ხელს უშლის მოდელის ზედმეტად თავდაჯერებულობას თავის პროგნოზებში, რაც მოქმედებს როგორც რეგულარიზაციის ფორმა.
ოპტიმიზატორი (AdamW): Adam ოპტიმიზატორი შეიცვალა AdamW-ით. AdamW უკეთესად ამუშავებს weight decay-ს, რაც აუმჯობესებს რეგულარიზაციას.
სწავლის სიჩქარის განრიგი (OneCycleLR): ReduceLROnPlateau-ის ნაცვლად გამოიყენება optim.lr_scheduler.OneCycleLR. ეს არის ძალიან ძლიერი და დინამიური განრიგი, რომელიც ცვლის სწავლის სიჩქარეს და მომენტუმს მთელი ტრენინგის ციკლის განმავლობაში, ხშირად იწვევს უფრო სწრაფ კონვერგენციას და უკეთეს შედეგებს. scheduler.step() ახლა სრულდება თითოეული ბლოკის შემდეგ, რაც სწორია OneCycleLR-ისთვის.
რატომ 65%?

65% სიზუსტე (66%-ის ნაცვლად) შესაძლოა გამოწვეული იყოს რამდენიმე ფაქტორის კომბინაციით:

"Skip Connection"-ის მოხსნა: ეს არის ყველაზე სავარაუდო არქიტექტურული მიზეზი.
მეტად აგრესიული რეგულარიზაცია/აგმენტაცია: თუმცა ზოგადად სასარგებლოა, თუ მონაცემთა ნაკრები არ არის საკმარისად დიდი, გადაჭარბებულმა რეგულარიზაციამ შეიძლება მოდელს ხელი შეუშალოს საკმარისი ნიმუშების სწავლაში.
OneCycleLR-ის ოპტიმალური პარამეტრები: მიუხედავად მისი ძლიერებისა, OneCycleLR-ს სჭირდება ზუსტი ტიუნინგი, განსაკუთრებით max_lr პარამეტრი, რათა თავიდან იქნას აცილებული არასტაბილურობა ტრენინგის დროს.
მიუხედავად 1%-იანი ვარდნისა, ეს მოდელი მაინც წარმოადგენს მნიშვნელოვან წინსვლას და აჩვენებს სხვადასხვა არქიტექტურული და ტრენინგის სტრატეგიების გავლენას საბოლოო შედეგზე.
